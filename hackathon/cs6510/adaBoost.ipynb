{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import sys\n",
    "from sklearn.impute import SimpleImputer\n",
    "import random\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    \n",
    "    \n",
    "    train = train[~(train['num_of_cancelled_trips'] >= 6)]\n",
    "    train = train[~(train['anon_var_1'] >=115)]\n",
    "    train = train[~(train['anon_var_2'] >= 70)]\n",
    "    train = train[~(train['anon_var_3'] >= 110 )]\n",
    "    train = train[~(2>train['customer_score'])]\n",
    "    train = train[~(train['customer_score']>=3.7 )]\n",
    "    print(train.columns)\n",
    "    return train\n",
    "\n",
    "def hotenc(train):\n",
    "    mf1 = pd.get_dummies(train['taxi_type'])\n",
    "    train = train.drop('taxi_type',axis = 1)\n",
    "    train = train.join(mf1,rsuffix='_Taxitype')\n",
    "\n",
    "    mf2 = pd.get_dummies(train['customer_score_confidence'])\n",
    "    train = train.drop('customer_score_confidence',axis = 1)\n",
    "    train = train.join(mf2,rsuffix='_Customerscore_confidence')\n",
    "\n",
    "    mf = pd.get_dummies(train['drop_location_type'])\n",
    "    train = train.drop('drop_location_type',axis = 1)\n",
    "    train = train.join(mf,rsuffix='_drop_location_type')\n",
    "\n",
    "    mf = pd.get_dummies(train['sex'])\n",
    "    train = train.drop('sex',axis = 1)\n",
    "    train = train.join(mf,rsuffix='sex')\n",
    "    mf = train['anon_var_1'].isnull()\n",
    "    mf =mf.astype(int)\n",
    "    print (mf)\n",
    "    train = train.join(mf,rsuffix= 'anon_var_1_presence')\n",
    "    \n",
    "#     train = train.drop('sex',axis = 1)\n",
    "#     train = train.drop('distance',axis = 1)\n",
    "    train = train.drop('anon_var_1',axis = 1)\n",
    "    train = train.drop('customer_score',axis = 1)\n",
    "#     train = train.drop('ratings_given_by_cust',axis = 1)\n",
    "#     train = train.drop('anon_var_3',axis = 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train\n",
    "\n",
    "def imputer(data,test):\n",
    "    \n",
    "    imp_mean1 = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_mean2 = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_median = SimpleImputer(missing_values=np.nan, fill_value = -1)\n",
    "\n",
    "    \n",
    "#     dat = np.array(data[\"customer_score\"])\n",
    "#     imp_mean1.fit(np.resize(dat,(len(dat),1)))\n",
    "#     dat = imp_mean1.transform(np.resize(dat,(len(dat),1)))\n",
    "#     data[\"customer_score\"] = dat\n",
    "# #     print(data.isna().sum())\n",
    "\n",
    "    \n",
    "#     dat = np.array(test[\"customer_score\"])\n",
    "#     dat = imp_mean1.transform(np.resize(dat,(len(dat),1)))\n",
    "#     test[\"customer_score\"] = dat\n",
    "#     print(test.isna().sum())\n",
    "\n",
    "#     dat = np.array(data[\"anon_var_1\"])\n",
    "#     imp_mean2.fit(np.resize(dat,(len(dat),1)))\n",
    "#     dat = imp_mean2.transform(np.resize(dat,(len(dat),1)))\n",
    "#     data[\"anon_var_1\"] = dat\n",
    "# #     print(data.isna().sum())\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dat = np.array(test[\"anon_var_1\"])\n",
    "#     dat = imp_mean1.transform(np.resize(dat,(len(dat),1)))\n",
    "#     test[\"anon_var_1\"] = dat\n",
    "    \n",
    "    \n",
    "    dat = np.array(data[\"months_of_activity\"])\n",
    "    imp_median.fit(np.resize(dat,(len(dat),1)))\n",
    "    dat = imp_median.transform(np.resize(dat,(len(dat),1)))\n",
    "    data[\"months_of_activity\"] = dat\n",
    "#     print(data.isna().sum())\n",
    "    \n",
    "    \n",
    "    \n",
    "    dat = np.array(test[\"months_of_activity\"])\n",
    "    dat = imp_median.transform(np.resize(dat,(len(dat),1)))\n",
    "    test[\"months_of_activity\"] = dat\n",
    "\n",
    "\n",
    "    \n",
    "#     print(test)\n",
    "    return data, test\n",
    "\n",
    "def boxCoxTransform(train, test):\n",
    "  \n",
    "    xt, _ = stats.boxcox(train[\"anon_var_2\"])\n",
    "    xte = stats.boxcox(test[\"anon_var_2\"],lmbda = _)\n",
    "    train[\"anon_var_2\"] = xt#(xt-xt.mean())/xt.std()\n",
    "    test[\"anon_var_2\"] = xte#(xte-xt.mean())/xt.std()\n",
    "    \n",
    "    xt, _ = stats.boxcox(train[\"distance\"])\n",
    "    xte = stats.boxcox(test[\"distance\"],lmbda = _)\n",
    "    train[\"distance\"] = xt #(xt-xt.mean())/xt.std()\n",
    "    test[\"distance\"] = xte #(xte-xt.mean())/xt.std()\n",
    "    \n",
    "#     xt, _ = stats.boxcox(train[\"customer_score\"])\n",
    "#     xte = stats.boxcox(test[\"customer_score\"],lmbda = _)\n",
    "#     train[\"customer_score\"] = xt#(xt-xt.mean())/xt.std()\n",
    "#     test[\"customer_score\"] = xte#(xte-xt.mean())/xt.std()\n",
    "\n",
    "    xt, _ = stats.boxcox(train[\"ratings_given_by_cust\"])\n",
    "    xte = stats.boxcox(test[\"ratings_given_by_cust\"],lmbda = _)\n",
    "    train[\"ratings_given_by_cust\"] = xt#(xt-xt.mean())/xt.std()\n",
    "    test[\"ratings_given_by_cust\"] = xte#(xte-xt.mean())/xt.std()\n",
    "    \n",
    "#     xt, _ = stats.boxcox(train[\"anon_var_1\"])\n",
    "#     xte = stats.boxcox(test[\"anon_var_1\"],lmbda = _)\n",
    "#     train[\"anon_var_1\"] = xt#(xt-xt.mean())/xt.std()\n",
    "#     test[\"anon_var_1\"] = xte#(xte-xt.mean())/xt.std()\n",
    "\n",
    "    \n",
    "    xt, _ = stats.boxcox(train[\"anon_var_3\"])\n",
    "    xte = stats.boxcox(test[\"anon_var_3\"],lmbda = _)\n",
    "    train[\"anon_var_3\"] = xt#(xt-xt.mean())/xt.std()\n",
    "    test[\"anon_var_3\"] = xte#(xte-xt.mean())/xt.std()\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'distance', 'taxi_type', 'months_of_activity', 'customer_score',\n",
      "       'customer_score_confidence', 'drop_location_type',\n",
      "       'ratings_given_by_cust', 'num_of_cancelled_trips', 'anon_var_1',\n",
      "       'anon_var_2', 'anon_var_3', 'sex', 'pricing_category'],\n",
      "      dtype='object')\n",
      "0        1\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        1\n",
      "5        1\n",
      "6        1\n",
      "7        1\n",
      "8        0\n",
      "9        1\n",
      "10       0\n",
      "11       1\n",
      "12       0\n",
      "13       0\n",
      "14       1\n",
      "15       0\n",
      "16       0\n",
      "17       0\n",
      "18       0\n",
      "19       1\n",
      "20       1\n",
      "21       1\n",
      "22       1\n",
      "23       1\n",
      "24       0\n",
      "25       0\n",
      "26       1\n",
      "27       0\n",
      "28       1\n",
      "29       1\n",
      "        ..\n",
      "78967    0\n",
      "78968    0\n",
      "78969    1\n",
      "78970    0\n",
      "78971    0\n",
      "78972    1\n",
      "78973    0\n",
      "78974    1\n",
      "78975    1\n",
      "78976    1\n",
      "78977    1\n",
      "78978    1\n",
      "78979    0\n",
      "78980    0\n",
      "78981    1\n",
      "78982    1\n",
      "78983    1\n",
      "78984    1\n",
      "78985    1\n",
      "78986    1\n",
      "78987    1\n",
      "78988    0\n",
      "78989    0\n",
      "78990    0\n",
      "78991    1\n",
      "78992    0\n",
      "78993    1\n",
      "78994    1\n",
      "78995    1\n",
      "78996    0\n",
      "Name: anon_var_1, Length: 77673, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_data = read_train()\n",
    "train_data = hotenc(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv(\"test.csv\")\n",
    "# test_data = hotenc(test_data)\n",
    "# train_data, test_data = imputer(train_data,test_data)\n",
    "# train_data, test_data = boxCoxTransform(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_train,data_test = train_test_split(train_data, test_size=0.2, shuffle = True)\n",
    "\n",
    "data_train, data_test = imputer(data_train,data_test)\n",
    "data_train, data_test = boxCoxTransform(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15535, 33)\n",
      "Index(['id', 'distance', 'months_of_activity', 'ratings_given_by_cust',\n",
      "       'num_of_cancelled_trips', 'anon_var_2', 'anon_var_3',\n",
      "       'pricing_category', 'A', 'B', 'C', 'D', 'E',\n",
      "       'A_Customerscore_confidence', 'B_Customerscore_confidence',\n",
      "       'C_Customerscore_confidence', 'A_drop_location_type',\n",
      "       'B_drop_location_type', 'C_drop_location_type', 'D_drop_location_type',\n",
      "       'E_drop_location_type', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
      "       'Female', 'Male', 'anon_var_1anon_var_1_presence'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data_test.shape)\n",
    "print(data_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    \n",
    "    clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "def BernoulliNb(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    outcome2 = clf.predict(xtrain)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    accuracy2 = accuracy_score(ytrain,outcome2)\n",
    "    print(accuracy,accuracy2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.681943997425169 0.6833982426212624\n"
     ]
    }
   ],
   "source": [
    "BernoulliNb(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def Multinomial(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    outcome2 = clf.predict(xtrain)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    accuracy2 = accuracy_score(ytrain,outcome2)\n",
    "    print(accuracy,accuracy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-f156f9448686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-109-a1e7d7049331>\u001b[0m in \u001b[0;36mMultinomial\u001b[0;34m(training_data, testing_data)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    608\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    609\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "Multinomial(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "def ExtraTreeClassy(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = ExtraTreesClassifier(n_estimators = 500,n_jobs = -1, min_impurity_decrease=0.0077)\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    outcome2 = clf.predict(xtrain)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    accuracy2 = accuracy_score(ytrain,outcome2)\n",
    "    print(accuracy,accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6740263920180238 0.6720847146673533\n"
     ]
    }
   ],
   "source": [
    "ExtraTreeClassy(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "def CompNb(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = ComplementNB()\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    outcome2 = clf.predict(xtrain)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    accuracy2 = accuracy_score(ytrain,outcome2)\n",
    "    print(accuracy,accuracy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-bbb311928a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCompNb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-4930ae64843b>\u001b[0m in \u001b[0;36mCompNb\u001b[0;34m(training_data, testing_data)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    608\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    609\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;34m\"\"\"Count feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "CompNb(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def LogReg(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = LogisticRegression(solver = \"saga\", max_iter = 1000,multi_class = 'multinomial',n_jobs = -1)\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    outcome = clf.predict(xtest)\n",
    "    outcome2 = clf.predict(xtrain)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    accuracy2 = accuracy_score(ytrain,outcome2)\n",
    "    print(accuracy,accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6934663662697136 0.6950014483890695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "LogReg(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "def LogRegcv(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = LogisticRegressionCV(solver = \"newton-cg\", max_iter = 200,multi_class = 'multinomial',n_jobs = -1)\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6943031863533956\n"
     ]
    }
   ],
   "source": [
    "LogRegcv(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "def MLP(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = MLPClassifier(activation = \"logistic\",solver = \"sgd\",learning_rate = 'adaptive', max_iter = 400)\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "def NearCentroid(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = NearestCentroid()\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NearCentroid(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "def RadNeighbour(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = RadiusNeighborsClassifier()\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadNeighbour(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "def Ridgereg(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = RidgeClassifier(alpha = 1,normalize=True,solver = 'sag')\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridgereg(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "def RidgeregCV(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = RidgeClassifierCV(normalize=True)\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RidgeregCV(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "def QuadraticDiscriminantAna(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = QuadraticDiscriminantAnalysis()\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QuadraticDiscriminantAna(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "def SGDClassy(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = SGDClassifier(loss='modified_huber',penalty='elasticnet',alpha = 0.001,n_jobs = -1,fit_intercept=True)\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDClassy(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "def PassiveAggressiveClass(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    \n",
    "    clf = PassiveAggressiveClassifier(n_iter = 200,fit_intercept=True,tol = 0.0001,validation_fraction = 0.77,n_jobs = -1)\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PassiveAggressiveClass(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RandomForestClassi(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "#    \n",
    "    clf = RandomForestClassifier( n_estimators = 500, criterion = 'gini',max_depth = 8, n_jobs = -1,oob_score = True,bootstrap = 0.77,max_features='sqrt')\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassi(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# def Voting(training_data,testing_data):\n",
    "training_data = data_train.copy()\n",
    "testing_data = data_test.copy()\n",
    "\n",
    "\n",
    "ytrain = training_data[\"pricing_category\"]\n",
    "xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "\n",
    "ytest = testing_data[\"pricing_category\"]\n",
    "xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "xtest = xtest.drop(\"id\",axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "clf1 = BernoulliNB()\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "#clf2 = MultinomialNB()\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "# clf2 = ExtraTreesClassifier(n_estimators = 200,n_jobs = -1)\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "clf3 = ComplementNB()\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "clf4 = LogisticRegression(solver = \"saga\", max_iter = 1000,multi_class = 'multinomial',n_jobs = -1)\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "\n",
    "# clf5 = LogisticRegressionCV(solver = \"newton-cg\", max_iter = 200,multi_class = 'multinomial',n_jobs = -1)\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "\n",
    "clf6 = MLPClassifier(activation = \"logistic\",solver = \"sgd\",learning_rate = 'adaptive', max_iter = 400)\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "\n",
    "clf7 = RidgeClassifier(alpha = 1,normalize=True,solver = 'sag')\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "\n",
    "# clf8 = RidgeClassifierCV(normalize=True)\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# clf9 = QuadraticDiscriminantAnalysis()\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "clf10 = SGDClassifier(loss='modified_huber',penalty='elasticnet',alpha = 0.001,n_jobs = -1,fit_intercept=True)\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "#     print(accuracy)\n",
    "\n",
    "\n",
    "clf11 = PassiveAggressiveClassifier(n_iter = 200,fit_intercept=True,tol = 0.0001,validation_fraction = 0.77,n_jobs = -1)\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "#     print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf12 = RandomForestClassifier( n_estimators = 500, criterion = 'gini',max_depth = 8, n_jobs = -1,oob_score = True,bootstrap = 0.77,max_features='sqrt')\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     outcome = clf.predict(xtest)\n",
    "#     accuracy = accuracy_score(ytest,outcome)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf13 = XGBClassifier(nthread=-1,max_depth = 6, learning_rate=0.077,n_estimators=350,subsample=0.77,colsample_bynode=0.75)\n",
    "\n",
    "\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('1', clf1), ('12',clf12),('13',clf13)],voting='soft',n_jobs=-1)\n",
    "# ('10',clf10)\n",
    "# ('3', clf3), ('2', clf2),(5',clf5),('8', clf8)('9', clf9),#,('6',clf6),('7', clf7),('7', clf7), ('11',clf11) ,  ('10',clf10),('11',clf11),('12',clf12),('13',clf13)], \n",
    "eclf.fit(xtrain, ytrain)\n",
    "\n",
    "outcome = eclf.predict(xtest)\n",
    "accuracy = accuracy_score(ytest,outcome)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Voting(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bayesian Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udit/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/udit/anaconda3/lib/python3.7/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "training_data = data_train.copy()\n",
    "testing_data = data_test.copy()\n",
    "\n",
    "ytrain = training_data[\"pricing_category\"]\n",
    "xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "\n",
    "ytest = testing_data[\"pricing_category\"]\n",
    "xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "xtest = xtest.drop(\"id\",axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(xtrain, label=ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-e69a9057ffca>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-e69a9057ffca>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    'objective':reg:logistic}\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def xgb_evaluate(learning_rate,max_depth, gamma, colsample_bytree,n_estimators):\n",
    "    params = {'learning_rate': learning_rate,'max_depth': int(max_depth),'subsample': 0.8,'eta': 0.1,\n",
    "              'gamma': gamma,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "             'n_estimators' : n_estimators,\n",
    "             'objective':reg:logistic}\n",
    "    # Used around 1000 boosting rounds in the full model\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=400, nfold=3)    \n",
    "    \n",
    "    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
    "    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "b'[16:16:32] src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression\\n\\nStack trace returned 10 entries:\\n[bt] (0) /home/udit/anaconda3/lib/libxgboost.so(dmlc::StackTrace[abi:cxx11](unsigned long)+0x27a) [0x7fdb0f9b487a]\\n[bt] (1) /home/udit/anaconda3/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x40) [0x7fdb0f9b57e0]\\n[bt] (2) /home/udit/anaconda3/lib/libxgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticRegression>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x573) [0x7fdb0fa6f5b3]\\n[bt] (3) /home/udit/anaconda3/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x1f9) [0x7fdb0f9c9cc9]\\n[bt] (4) /home/udit/anaconda3/lib/libxgboost.so(XGBoosterUpdateOneIter+0x48) [0x7fdb0fb4d228]\\n[bt] (5) /home/udit/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fdb3ee17ec0]\\n[bt] (6) /home/udit/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fdb3ee1787d]\\n[bt] (7) /home/udit/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fdb3f02df7e]\\n[bt] (8) /home/udit/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x139b4) [0x7fdb3f02e9b4]\\n[bt] (9) /home/udit/anaconda3/bin/python(_PyObject_FastCallKeywords+0x49b) [0x562cffcccd2b]\\n\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.5781079544900427, 0.05714355003370919, 0.16694192369711386, 5.158415581057396, 493.66055478215816)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0fa9399dd1a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Use the expected improvement acquisition function to handle negative numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Optimally needs quite a few more initiation points and number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxgb_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ei'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-ff4595e4b95e>\u001b[0m in \u001b[0;36mxgb_evaluate\u001b[0;34m(learning_rate, max_depth, gamma, colsample_bytree, n_estimators)\u001b[0m\n\u001b[1;32m      6\u001b[0m              'objective':'reg:logistic'}\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Used around 1000 boosting rounds in the full model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcv_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[1;32m    443\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \"\"\"\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[16:16:32] src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression\\n\\nStack trace returned 10 entries:\\n[bt] (0) /home/udit/anaconda3/lib/libxgboost.so(dmlc::StackTrace[abi:cxx11](unsigned long)+0x27a) [0x7fdb0f9b487a]\\n[bt] (1) /home/udit/anaconda3/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x40) [0x7fdb0f9b57e0]\\n[bt] (2) /home/udit/anaconda3/lib/libxgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticRegression>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x573) [0x7fdb0fa6f5b3]\\n[bt] (3) /home/udit/anaconda3/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x1f9) [0x7fdb0f9c9cc9]\\n[bt] (4) /home/udit/anaconda3/lib/libxgboost.so(XGBoosterUpdateOneIter+0x48) [0x7fdb0fb4d228]\\n[bt] (5) /home/udit/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fdb3ee17ec0]\\n[bt] (6) /home/udit/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fdb3ee1787d]\\n[bt] (7) /home/udit/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fdb3f02df7e]\\n[bt] (8) /home/udit/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x139b4) [0x7fdb3f02e9b4]\\n[bt] (9) /home/udit/anaconda3/bin/python(_PyObject_FastCallKeywords+0x49b) [0x562cffcccd2b]\\n\\n'"
     ]
    }
   ],
   "source": [
    "xgb_bo = BayesianOptimization(xgb_evaluate, {'learning_rate': (0.01, 1.0),'max_depth': (3, 7), 'gamma': (0, 1),'colsample_bytree': (0.3, 0.9),'n_estimators': (200, 500)})\n",
    "# Use the expected improvement acquisition function to handle negative numbers\n",
    "# Optimally needs quite a few more initiation points and number of iterations\n",
    "xgb_bo.maximize(init_points=3, n_iter=10, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'target': -0.5976786666666666, 'params': {'colsample_bytree': 0.6942893223805829, 'gamma': 0.06070609983581221, 'learning_rate': 0.4153263867311826, 'max_depth': 6.98651903146903, 'n_estimators': 307.75442499046574}}\n",
      " {'target': -0.5893056666666666, 'params': {'colsample_bytree': 0.5210342064222988, 'gamma': 0.7437199296558867, 'learning_rate': 0.6845914451567847, 'max_depth': 4.3916800036527075, 'n_estimators': 371.25962116207006}}\n",
      " {'target': -0.563813, 'params': {'colsample_bytree': 0.36942119269796003, 'gamma': 0.06792204242706901, 'learning_rate': 0.0447667829037498, 'max_depth': 6.605016240503023, 'n_estimators': 443.1736203745203}}\n",
      " {'target': -0.6200313333333333, 'params': {'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 7.0, 'n_estimators': 500.0}}\n",
      " {'target': -0.6526473333333334, 'params': {'colsample_bytree': 0.3, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 7.0, 'n_estimators': 200.0}}\n",
      " {'target': -0.70892, 'params': {'colsample_bytree': 0.8182732416418981, 'gamma': 0.6462894084490496, 'learning_rate': 0.9940435517353323, 'max_depth': 6.99770454094158, 'n_estimators': 430.754891267528}}\n",
      " {'target': -0.6646933333333332, 'params': {'colsample_bytree': 0.3, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 3.0, 'n_estimators': 466.8904154856992}}\n",
      " {'target': -0.5846766666666666, 'params': {'colsample_bytree': 0.3, 'gamma': 1.0, 'learning_rate': 1.0, 'max_depth': 3.0, 'n_estimators': 252.1330891966494}}\n",
      " {'target': -0.6646933333333332, 'params': {'colsample_bytree': 0.3, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 3.0, 'n_estimators': 342.7470077406105}}\n",
      " {'target': -0.6200313333333333, 'params': {'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 7.0, 'n_estimators': 278.57140137736883}}\n",
      " {'target': -0.6200313333333333, 'params': {'colsample_bytree': 0.9, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 7.0, 'n_estimators': 229.09066400924968}}\n",
      " {'target': -0.6526473333333334, 'params': {'colsample_bytree': 0.3, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 7.0, 'n_estimators': 451.13466461810475}}\n",
      " {'target': -0.6646933333333332, 'params': {'colsample_bytree': 0.3, 'gamma': 0.0, 'learning_rate': 0.01, 'max_depth': 3.0, 'n_estimators': 442.17464184909903}}]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(xgb_bo.res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['distance', 'months_of_activity', 'customer_score',\n",
      "       'ratings_given_by_cust', 'num_of_cancelled_trips', 'anon_var_1',\n",
      "       'anon_var_2', 'anon_var_3', 'A', 'B', 'C', 'D', 'E',\n",
      "       'A_Customerscore_confidence', 'B_Customerscore_confidence',\n",
      "       'C_Customerscore_confidence', 'A_drop_location_type',\n",
      "       'B_drop_location_type', 'C_drop_location_type', 'D_drop_location_type',\n",
      "       'E_drop_location_type', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
      "       'Female', 'Male'],\n",
      "      dtype='object')\n",
      "Index(['distance', 'months_of_activity', 'customer_score',\n",
      "       'ratings_given_by_cust', 'num_of_cancelled_trips', 'anon_var_1',\n",
      "       'anon_var_2', 'anon_var_3', 'A', 'B', 'C', 'D', 'E',\n",
      "       'A_Customerscore_confidence', 'B_Customerscore_confidence',\n",
      "       'C_Customerscore_confidence', 'A_drop_location_type',\n",
      "       'B_drop_location_type', 'C_drop_location_type', 'D_drop_location_type',\n",
      "       'E_drop_location_type', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
      "       'Female', 'Male'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(xtest.columns)\n",
    "print(xtrain.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = xgb_bo.res[2]['params']\n",
    "params['max_depth'] = int(params['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5769142530676593\n"
     ]
    }
   ],
   "source": [
    "model2 = xgb.train(params, dtrain, num_boost_round=400)\n",
    "\n",
    "\n",
    "tets = xgb.DMatrix(xtest)\n",
    "\n",
    "# Predict on testing and training set\n",
    "y_pred = model2.predict(tets)\n",
    "# y_train_pred = model2.predict(xtrain)\n",
    "\n",
    "# Report testing and training RMSE\n",
    "print(np.sqrt(mean_squared_error(ytest, y_pred)))\n",
    "# print(np.sqrt(mean_squared_error(ytrain, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def AdaBoostClass(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "#  \n",
    "\n",
    "    clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=600,learning_rate=1,algorithm=\"SAMME\")\n",
    "#     clf = RandomForestClassifier( n_estimators = 500, criterion = 'gini',max_depth = 8, n_jobs = -1,oob_score = True,bootstrap = 0.77,max_features='sqrt')\n",
    "#     clf = OneVsRestClassifier(SVC(kernel = \"poly\"),n_jobs =-1)\n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    outcome2 = clf.predict(xtrain)\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    accuracy2 = accuracy_score(ytrain,outcome2)\n",
    "    print(accuracy,accuracy2)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     bdt_real = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=600,learning_rate=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6930157708400386 0.6938266439215939\n"
     ]
    }
   ],
   "source": [
    "AdaBoostClass(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xg(training_data,testing_data):\n",
    "    ytrain = training_data[\"pricing_category\"]\n",
    "    xtrain = training_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtrain = xtrain.drop(\"id\",axis = 1)\n",
    "    \n",
    "    ytest = testing_data[\"pricing_category\"]\n",
    "    xtest = testing_data.drop(\"pricing_category\",axis = 1)\n",
    "    xtest = xtest.drop(\"id\",axis = 1)\n",
    "    clf = XGBClassifier(nthread=-1,max_depth = 4, learning_rate=0.01,n_estimators=5000,subsample=0.77,colsample_bynode=0.75,verbosity = 0)\n",
    "    \n",
    "    clf.fit(xtrain, ytrain)\n",
    "\n",
    "    #     polynomial_svm = SVC(C = c, kernel = \"poly\",degree = deg,coef0 = 1, gamma = 1)\n",
    "\n",
    "    #     polynomial_svm.fit(x,y)\n",
    "    \n",
    "    outcome = clf.predict(xtest)\n",
    "    \n",
    "    \n",
    "\n",
    "    accuracy = accuracy_score(ytest,outcome)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6996459607338268\n"
     ]
    }
   ],
   "source": [
    "xg(data_train,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
